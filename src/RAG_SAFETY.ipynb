{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GREtTc5j6peh"
   },
   "outputs": [],
   "source": [
    "from notion_client import Client\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq7-Ktxiz_Qg"
   },
   "outputs": [],
   "source": [
    "def fetch_notion_page(client: Client, page_id: str) -> list[Document]:\n",
    "    docs = []\n",
    "    def recurse(pid):\n",
    "        res = client.blocks.children.list(block_id=pid)\n",
    "        texts = []\n",
    "        for block in res[\"results\"]:\n",
    "            t = block[\"type\"]\n",
    "            if t in (\"paragraph\",\"heading_1\",\"heading_2\",\"heading_3\",\n",
    "                     \"bulleted_list_item\",\"numbered_list_item\",\"quote\",\"code\"):\n",
    "                rich = block[t].get(\"rich_text\", [])\n",
    "                texts.append(\"\".join(r[\"plain_text\"] for r in rich))\n",
    "            elif t == \"child_page\":\n",
    "                texts.append(f\"# {block['child_page']['title']}\")\n",
    "                recurse(block[\"id\"])\n",
    "        docs.append(Document(\n",
    "            page_content=\"\\n\\n\".join(texts),\n",
    "            metadata={\"source\": pid}\n",
    "        ))\n",
    "    recurse(page_id)\n",
    "    return docs\n",
    "\n",
    "def ingest_notion_page(\n",
    "    integration_token: str,\n",
    "    page_id: str,\n",
    "    persist_dir: str = \"./ais_notion_chroma_db\",\n",
    "):\n",
    "    # 1) Fetch raw pages\n",
    "    client = Client(auth=integration_token)\n",
    "    pages = fetch_notion_page(client, page_id)\n",
    "\n",
    "    # 2) Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024, chunk_overlap=100, length_function=len, add_start_index=True\n",
    "    )\n",
    "    docs = splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} pages into {len(docs)} chunks.\")\n",
    "\n",
    "    # 3) Embed & persist\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226,
     "referenced_widgets": [
      "b7ba17e8c3954784bf902ac3536c0e79",
      "da6b3d2b57e5485c8247c4c1ae0f8d40",
      "5d5584c0a024486c874fc9f46e247b72",
      "e4e998d839fd45b28732cc384e1497f8",
      "17313607d97f49f2ae7fc56aa93e9569",
      "35a15eb57be54b0db97f4df81db2f495",
      "2bd48505c10145b1813947476344a101",
      "2dd82f9295f84c1b86092a75cb28b8f7",
      "f733d2494da648978c61a35e743418a8",
      "90f0f270525e4f6f993cc554124310fd",
      "35b1c1b0c3264d54afe37fde615321cd",
      "ae3c60f65d6c4103916f52b98f7ff7b3",
      "ad5f87dd3016497bb9bf1d8afe9179d1",
      "e4104fa50b4b408d825b65c41ab283bd",
      "664832db01f348749798c8e49a106431",
      "7aea59c04f9d442491ffd360db606630",
      "e27b37054f57433ebb2144638a8b8373",
      "acc9b5934b804b7fad94f913b285a4e7",
      "5ff81b10c18549ffa6ee178460a4d6bd",
      "ebf9b543486b44c4abf1f5aa5cab3cf8",
      "8ee7668b445f4fedbb0c4f689fdd7caa",
      "0ffd01efef5a4b93a163658961b4cbb8",
      "51ef68789c9b4015a8e6c5465863a0db",
      "e1c29d10445a4f338e8b6e78e59d2ee6",
      "fb8b7e5b9a054e8aaade716711f6bb15",
      "f4f9e304f0f445a38d73f043c981ce8c",
      "fd447dd8094a4d91a76936f6e234fcba",
      "932d4ef189a548678ad292ca4c398600",
      "70d6d800f4c544738d85398617fd55b7",
      "2f37960a78484fbe9a4c53c0fc2c83bc",
      "b507230d3ae346cab3e974d5f79a2459",
      "276e9509528348aa921fe1f5564a2665",
      "28012551bfee4d37943c25f0fe8f4e9d",
      "560a66dc94864b2996127385cf117917",
      "5931f92334f9476dba1aef545e0732e8",
      "4371205582614c87832d648975bae980",
      "da02fe1a59704d42955f5dd8978d7905",
      "f928c8000ffb46d8bf46b57be7c0196e",
      "1558d61a7cda409dae79e5d0255b0989",
      "05362173d07e435c99f6b679a68badef",
      "c111151c900d444d985049ae790c09db",
      "a12912147c6646759268b0897489258f",
      "b5b8368b92c64e36ae784a65e6c6837f",
      "589e31449d264f9a863a07ea2a01e967",
      "798e5cef368e44e5862ca4812b436e20",
      "9289c4f271dd44c390ed2fa481e2cfc4",
      "d4ec5c86044744d187c691ee8a4c3137",
      "87f4e60cb23646f4b056b1ae1cacde72",
      "98306369152e4d1dbf8da39a1140e8ca",
      "e256fd480ffb455d890c0e7ccdd76ee9",
      "bc3fd5cff69f4ae9998e4dea3e33107b",
      "36c18ff0e9e24064b33cf12571905759",
      "8babca47bfff43eca8d520066bd69573",
      "561404f7c4a044688500f2d541b76018",
      "cce7457417404de9a8ba92000b0c9ec8",
      "4bfa5cc8b4dc41068d1e1797d4f56efd",
      "15ad9a4850594cf89404476b2e2584c3",
      "77ea5897c3ba476a8b2acfc4392c8d1e",
      "425306cf799547fba69e6f7cf6a8f3e4",
      "18b28c0eb78b49049ce6cf14cb512202",
      "1b2fa5a84fc24dbd855261a397345ade",
      "55a990d24d7b4b998551c6d94bf400b2",
      "0c7288e9112747ee99692524ac7c17c9",
      "1d69a35692ae497c8eb4ec3e84e00428",
      "30de82c752d74b3e84a57bc464bb8771",
      "f2308870e10744ee814e87e2d56efa8b"
     ]
    },
    "id": "RAdst-000D41",
    "outputId": "f75c0140-beda-42f5-b298-c03da5c9af46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 50 pages into 284 chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ba17e8c3954784bf902ac3536c0e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3c60f65d6c4103916f52b98f7ff7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ef68789c9b4015a8e6c5465863a0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560a66dc94864b2996127385cf117917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798e5cef368e44e5862ca4812b436e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfa5cc8b4dc41068d1e1797d4f56efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ingest_notion_page(\"integration_token\", \"5b9ffa2c525f481ea08ed801d8ef7896\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOvQpMkR7biZ"
   },
   "outputs": [],
   "source": [
    "def rag_chain_openai():\n",
    "    model = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        [Instructions] You are a helpful assistant. Answer from only the context.\n",
    "        If you don’t know, reply “No context available for: {input}”.\n",
    "        Question: {input}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=\"./ais_notion_chroma_db\",\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    doc_chain = create_stuff_documents_chain(model, prompt)\n",
    "    return create_retrieval_chain(retriever, doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvxMeMXx7iu0"
   },
   "outputs": [],
   "source": [
    "chain = rag_chain_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AprDL36ln6Z2",
    "outputId": "4fc4d397-13f1-4129-c8e8-1a12277f3ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Whats a cost function ?', 'context': [Document(id='0e938cb5-3763-4981-93a7-431c57ab8ba0', metadata={'start_index': 0, 'source': '1752599b-3926-42d3-bdde-e5c293a4f3bb'}, page_content=\"Cost functions\\n\\nFor the neural network to learn we define cost functions. This indicates to the system what is a good answer and what isn't. For the example in particular if the function correctly classifies the image we should get the activation of one of the neurons very high and all the others very low. In the case of getting high values for different numbers, we have a high cost and don't like the answer.\\n\\nThe cost is defined as the sum of the squared differences between what the system gave me and what I expected. By averaging the costs we can get an idea of how well is the network performing.\\n\\nWe can think of the cost function as a function that receives only one parameter and we want to minimize it. Starting from a random input and getting into a local minimum can be feasible but getting to the global meaning is extremely hard.\"), Document(id='f440f163-9b02-4099-a36d-0aa330d66d16', metadata={'source': '1752599b-3926-42d3-bdde-e5c293a4f3bb', 'start_index': 849}, page_content=\"If we think of the cost as a multivariable function we could think that adding a small step of the negative of the gradient to the actual gradient of the cost will slowly provide us with the direction to get to a local minimum.\\n\\nThe algorithm for computing the gradient efficiently is called backpropagation.\\n\\nLearning → Minimizing a cost function.\\n\\nLearning\\n\\nThe learning for this type of neural network doesn't detect patterns to recognize images. It sees all of the different pixels and analyzes them to better fit the cost function.\"), Document(id='486257f4-d516-4998-9559-8322e4e46306', metadata={'source': 'f9d04b9f-2951-430d-ab4d-14ff26ba366d', 'start_index': 933}, page_content=\"We also could want to only detect significant input if we have a certain degree of certainty that the image is what we want. We add a bias into the sigmoid function that makes it so we don't accept an image that is not clearly what we want.\\n\\nBetter notation\\n\\nFor a better notation, we could use a matrix and vectors and do the vector product to find the answer.\\n\\nBetter Function\\n\\nUsing sigmoid was slow to learn so they changed it for ReLU which is a function that gives 0 for negative values and the identity for positive ones.\\n\\nLearning\\n\\nLearning → Finding the right weights and biases so it solves the problem at hand.\")], 'answer': \"A cost function is a function that indicates to the system what is a good answer and what isn't by measuring the sum of squared differences between the system's output and the expected output. It is used to evaluate how well a neural network is performing and is minimized during the learning process.\"}\n",
      "dict_keys(['input', 'context', 'answer'])\n",
      "ANSWER: A cost function is a function that indicates to the system what is a good answer and what isn't by measuring the sum of squared differences between the system's output and the expected output. It is used to evaluate how well a neural network is performing and is minimized during the learning process.\n",
      "→ Source: 1752599b-3926-42d3-bdde-e5c293a4f3bb\n",
      "→ Source: 1752599b-3926-42d3-bdde-e5c293a4f3bb\n",
      "→ Source: f9d04b9f-2951-430d-ab4d-14ff26ba366d\n"
     ]
    }
   ],
   "source": [
    "resp = chain.invoke({\"input\": \"Whats a cost function ?\"})\n",
    "print(resp)\n",
    "print(resp.keys())\n",
    "\n",
    "print(\"ANSWER:\", resp[\"answer\"])\n",
    "for doc in resp[\"context\"]:\n",
    "    print(\"→ Source:\", doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzUfQpelsj-2"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def show_fancy(resp):\n",
    "    display(Markdown(f\"## 📝 Answer\\n\\n{resp['answer']}\"))\n",
    "\n",
    "    md = \"### 🔗 Sources\\n\"\n",
    "    for doc in resp[\"context\"]:\n",
    "        snippet = doc.page_content.replace(\"\\n\", \" \")[:120] + \"…\"\n",
    "        md += f\"- **{doc.metadata['source']}**: `{snippet}`\\n\"\n",
    "    display(Markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "Ac92I7N9spIY",
    "outputId": "7c7c742a-e622-4571-d381-46c6d3aac236"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 📝 Answer\n",
       "\n",
       "A cost function is a function that indicates to the system what is a good answer and what isn't by measuring the sum of squared differences between the system's output and the expected output. It is used to evaluate how well a neural network is performing and is minimized during the learning process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 🔗 Sources\n",
       "- **1752599b-3926-42d3-bdde-e5c293a4f3bb**: `Cost functions  For the neural network to learn we define cost functions. This indicates to the system what is a good an…`\n",
       "- **1752599b-3926-42d3-bdde-e5c293a4f3bb**: `If we think of the cost as a multivariable function we could think that adding a small step of the negative of the gradi…`\n",
       "- **f9d04b9f-2951-430d-ab4d-14ff26ba366d**: `We also could want to only detect significant input if we have a certain degree of certainty that the image is what we w…`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_fancy(resp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
